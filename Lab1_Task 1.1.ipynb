{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T21:20:51.216785Z",
     "start_time": "2025-04-12T21:20:48.566856Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load Data\n",
    "def load_data(file_path):\n",
    "    texts, labels = [], []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            text, label = line.strip().split('\\t')\n",
    "            texts.append(text)\n",
    "            labels.append(int(label))\n",
    "    return texts, labels\n",
    "\n",
    "file_small = r'C:\\Users\\Samous\\Downloads\\amazon_cells_labelled (1).txt'\n",
    "data, labels = load_data(file_small)\n",
    "\n",
    "# 2. Text to Vectors\n",
    "vectorizer = CountVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(data).toarray()\n",
    "\n",
    "# 3. Split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# 4. Tensor Conversion\n",
    "to_tensor = lambda x: torch.tensor(x, dtype=torch.float32)\n",
    "train_data, val_data, test_data = map(to_tensor, [X_train, X_val, X_test])\n",
    "train_labels = torch.tensor(y_train)\n",
    "val_labels = torch.tensor(y_val)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "# 5. Dataset & Dataloader\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data, self.labels = data, labels\n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self, idx): return self.data[idx], self.labels[idx]\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(TextDataset(train_data, train_labels), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TextDataset(val_data, val_labels), batch_size=batch_size)\n",
    "test_loader = DataLoader(TextDataset(test_data, test_labels), batch_size=batch_size)\n",
    "\n",
    "# 6. Define ANN\n",
    "class ANNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, output_dim=2):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    def forward(self, x): return self.model(x)\n",
    "\n",
    "model = ANNModel(input_dim=train_data.shape[1])\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 7. Training\n",
    "def train(model, loader):\n",
    "    model.train()\n",
    "    for inputs, labels in loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# 8. Evaluation\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return 100 * correct / total\n",
    "\n",
    "# 9. Run\n",
    "for epoch in range(10):\n",
    "    train(model, train_loader)\n",
    "    acc = evaluate(model, val_loader)\n",
    "    print(f\"Epoch {epoch+1}, Val Accuracy: {acc:.2f}%\")\n",
    "\n",
    "print(\"Test Accuracy:\", evaluate(model, test_loader))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Val Accuracy: 80.00%\n",
      "Epoch 2, Val Accuracy: 84.00%\n",
      "Epoch 3, Val Accuracy: 84.00%\n",
      "Epoch 4, Val Accuracy: 83.00%\n",
      "Epoch 5, Val Accuracy: 83.00%\n",
      "Epoch 6, Val Accuracy: 82.00%\n",
      "Epoch 7, Val Accuracy: 82.00%\n",
      "Epoch 8, Val Accuracy: 82.00%\n",
      "Epoch 9, Val Accuracy: 83.00%\n",
      "Epoch 10, Val Accuracy: 82.00%\n",
      "Test Accuracy: 86.0\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T21:33:54.640559Z",
     "start_time": "2025-04-12T21:33:31.341352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 1. Load Larger Dataset\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load Data\n",
    "def load_data(file_path):\n",
    "    texts, labels = [], []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            text, label = line.strip().split('\\t')\n",
    "            texts.append(text)\n",
    "            labels.append(int(label))\n",
    "    return texts, labels\n",
    "\n",
    "file_large =  r'C:\\Users\\Samous\\Desktop\\amazon_cells_labelled_LARGE_25K.txt'  # ‚Üê your 25K file path here\n",
    "\n",
    "data, labels = load_data(file_large)\n",
    "\n",
    "# 2. Text to Vectors\n",
    "vectorizer = CountVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(data).toarray()\n",
    "\n",
    "# 3. Split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# 4. Tensor Conversion\n",
    "to_tensor = lambda x: torch.tensor(x, dtype=torch.float32)\n",
    "train_data, val_data, test_data = map(to_tensor, [X_train, X_val, X_test])\n",
    "train_labels = torch.tensor(y_train)\n",
    "val_labels = torch.tensor(y_val)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "# 5. Dataset & Dataloader\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data, self.labels = data, labels\n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self, idx): return self.data[idx], self.labels[idx]\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(TextDataset(train_data, train_labels), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TextDataset(val_data, val_labels), batch_size=batch_size)\n",
    "test_loader = DataLoader(TextDataset(test_data, test_labels), batch_size=batch_size)\n",
    "\n",
    "# 6. Define ANN\n",
    "class ANNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, output_dim=2):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    def forward(self, x): return self.model(x)\n",
    "\n",
    "model = ANNModel(input_dim=train_data.shape[1])\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 7. Training\n",
    "def train(model, loader):\n",
    "    model.train()\n",
    "    for inputs, labels in loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# 8. Evaluation\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return 100 * correct / total\n",
    "\n",
    "# 9. Run\n",
    "for epoch in range(10):\n",
    "    train(model, train_loader)\n",
    "    acc = evaluate(model, val_loader)\n",
    "    print(f\"Epoch {epoch+1}, Val Accuracy: {acc:.2f}%\")\n",
    "\n",
    "print(\"Test Accuracy:\", evaluate(model, test_loader))\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Val Accuracy: 88.72%\n",
      "Epoch 2, Val Accuracy: 88.92%\n",
      "Epoch 3, Val Accuracy: 87.72%\n",
      "Epoch 4, Val Accuracy: 86.60%\n",
      "Epoch 5, Val Accuracy: 87.04%\n",
      "Epoch 6, Val Accuracy: 86.88%\n",
      "Epoch 7, Val Accuracy: 86.56%\n",
      "Epoch 8, Val Accuracy: 86.64%\n",
      "Epoch 9, Val Accuracy: 86.80%\n",
      "Epoch 10, Val Accuracy: 86.84%\n",
      "Test Accuracy: 87.28\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T21:44:19.303264Z",
     "start_time": "2025-04-12T21:43:59.212576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load Data from both small and large datasets\n",
    "def load_data(file_path):\n",
    "    texts, labels = [], []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            text, label = line.strip().split('\\t')\n",
    "            texts.append(text)\n",
    "            labels.append(int(label))\n",
    "    return texts, labels\n",
    "\n",
    "# Load small and large dataset\n",
    "file_small = r'C:\\Users\\Samous\\Downloads\\amazon_cells_labelled (1).txt'\n",
    "file_large = r'C:\\Users\\Samous\\Desktop\\amazon_cells_labelled_LARGE_25K.txt'  # Path to large file\n",
    "\n",
    "texts_small, labels_small = load_data(file_small)\n",
    "texts_large, labels_large = load_data(file_large)\n",
    "\n",
    "# 2. Combine small and large dataset\n",
    "texts = texts_small + texts_large\n",
    "labels = labels_small + labels_large\n",
    "\n",
    "# 3. Convert text to vectors using CountVectorizer (limiting to 5000 features)\n",
    "vectorizer = CountVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(texts).toarray()\n",
    "\n",
    "# 4. Split data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# 5. Convert data and labels to PyTorch tensors\n",
    "to_tensor = lambda x: torch.tensor(x, dtype=torch.float32)\n",
    "train_data, val_data, test_data = map(to_tensor, [X_train, X_val, X_test])\n",
    "train_labels = torch.tensor(y_train)\n",
    "val_labels = torch.tensor(y_val)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "# 6. Create a custom dataset class for loading batches\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data, self.labels = data, labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# 7. Set batch size and create DataLoader objects for training, validation, and test data\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(TextDataset(train_data, train_labels), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TextDataset(val_data, val_labels), batch_size=batch_size)\n",
    "test_loader = DataLoader(TextDataset(test_data, test_labels), batch_size=batch_size)\n",
    "\n",
    "# 8. Define the Artificial Neural Network (ANN) model\n",
    "class ANNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, output_dim=2):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = ANNModel(input_dim=train_data.shape[1])  # The input dimension is equal to the number of features\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 9. Define the training function\n",
    "def train(model, loader):\n",
    "    model.train()\n",
    "    for inputs, labels in loader:\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        loss = loss_fn(outputs, labels)  # Compute loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update model weights\n",
    "\n",
    "# 10. Define the evaluation function to compute accuracy\n",
    "def evaluate(model, loader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():  # No need to compute gradients during evaluation\n",
    "        for inputs, labels in loader:\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            _, preds = torch.max(outputs, 1)  # Get the predicted class\n",
    "            correct += (preds == labels).sum().item()  # Count correct predictions\n",
    "            total += labels.size(0)  # Total number of samples\n",
    "    return 100 * correct / total  # Return accuracy as a percentage\n",
    "\n",
    "# 11. Run training for 10 epochs and evaluate on the validation set\n",
    "for epoch in range(10):\n",
    "    train(model, train_loader)  # Train the model for one epoch\n",
    "    acc = evaluate(model, val_loader)  # Evaluate on the validation set\n",
    "    print(f\"Epoch {epoch+1}, Val Accuracy: {acc:.2f}%\")\n",
    "\n",
    "# 12. Final evaluation on the test set\n",
    "test_acc = evaluate(model, test_loader)\n",
    "print(\"Test Accuracy:\", test_acc)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Val Accuracy: 87.73%\n",
      "Epoch 2, Val Accuracy: 86.73%\n",
      "Epoch 3, Val Accuracy: 86.27%\n",
      "Epoch 4, Val Accuracy: 86.15%\n",
      "Epoch 5, Val Accuracy: 86.62%\n",
      "Epoch 6, Val Accuracy: 86.00%\n",
      "Epoch 7, Val Accuracy: 85.85%\n",
      "Epoch 8, Val Accuracy: 85.65%\n",
      "Epoch 9, Val Accuracy: 85.85%\n",
      "Epoch 10, Val Accuracy: 85.69%\n",
      "Test Accuracy: 86.0\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "colab": {
   "name": "SNN-Excercise-2_2021.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
