{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0383767e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53680c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\toste\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\toste\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from matplotlib import pyplot\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, classification_report\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_pandas(data, columns):\n",
    "    df_ = pd.DataFrame(columns=columns)\n",
    "    data['Sentence'] = data['Sentence'].str.lower()\n",
    "    data['Sentence'] = data['Sentence'].replace('[a-zA-Z0-9-_.]+@[a-zA-Z0-9-_.]+', '', regex=True)                      # remove emails\n",
    "    data['Sentence'] = data['Sentence'].replace('((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.|$)){4}', '', regex=True)    # remove IP address\n",
    "    data['Sentence'] = data['Sentence'].str.replace('[^\\w\\s]', '', regex=True)  # NEW (explicit)                                                      # remove special characters\n",
    "    data['Sentence'] = data['Sentence'].replace('\\d', '', regex=True)                                                   # remove numbers\n",
    "    for index, row in data.iterrows():\n",
    "        word_tokens = word_tokenize(row['Sentence'])\n",
    "        filtered_sent = [w for w in word_tokens if not w in stopwords.words('english')]\n",
    "        df_.loc[len(df_)] = {\n",
    "            \"index\": row['index'],\n",
    "            \"Class\": row['Class'],\n",
    "            \"Sentence\": \" \".join(filtered_sent)\n",
    "        }\n",
    "    return data\n",
    "\n",
    "# If this is the primary file that is executed (ie not an import of another file)\n",
    "if __name__ == \"__main__\":\n",
    "    # get data, pre-process and split\n",
    "    file_path = r\"C:\\Users\\toste\\Downloads\\amazon_cells_labelled.txt\"\n",
    "    data = pd.read_csv(file_path, delimiter='\\t', header=None)\n",
    "    data.columns = ['Sentence', 'Class']\n",
    "    data['index'] = data.index                                          # add new column index\n",
    "    columns = ['index', 'Class', 'Sentence']\n",
    "    data = preprocess_pandas(data, columns)                             # pre-process\n",
    "    training_data, validation_data, training_labels, validation_labels = train_test_split( # split the data into training, validation, and test splits\n",
    "        data['Sentence'].values.astype('U'),\n",
    "        data['Class'].values.astype('int32'),\n",
    "        test_size=0.10,\n",
    "        random_state=0,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    # vectorize data using TFIDF and transform for PyTorch for scalability\n",
    "    word_vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1,2), max_features=50000, max_df=0.5, use_idf=True, norm='l2')\n",
    "    training_data = word_vectorizer.fit_transform(training_data)        # transform texts to sparse matrix\n",
    "    training_data = training_data.todense()                             # convert to dense matrix for Pytorch\n",
    "    vocab_size = len(word_vectorizer.vocabulary_)\n",
    "    validation_data = word_vectorizer.transform(validation_data)\n",
    "    validation_data = validation_data.todense()\n",
    "    train_x_tensor = torch.from_numpy(np.array(training_data)).type(torch.FloatTensor)\n",
    "    train_y_tensor = torch.from_numpy(np.array(training_labels)).long()\n",
    "    validation_x_tensor = torch.from_numpy(np.array(validation_data)).type(torch.FloatTensor)\n",
    "    validation_y_tensor = torch.from_numpy(np.array(validation_labels)).long()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba775316",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e62c715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\toste\n",
      "                                                text  label\n",
      "0  So there is no way for me to plug it in here i...      0\n",
      "1                        Good case, Excellent value.      1\n",
      "2                             Great for the jawbone.      1\n",
      "3  Tied to charger for conversations lasting more...      0\n",
      "4                                  The mic is great.      1\n",
      "0    500\n",
      "1    500\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "\n",
    "file_path = r\"C:\\Users\\toste\\Downloads\\amazon_cells_labelled.txt\"\n",
    "df = pd.read_csv(file_path, sep=\"\\t\", header=None, names=[\"text\", \"label\"])\n",
    "\n",
    "# Step 2: View the first few rows\n",
    "print(df.head())\n",
    "\n",
    "# Optional: Check distribution of labels\n",
    "print(df['label'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc18ee25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple feedforward neural network\n",
    "class SentimentANN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64):\n",
    "        super(SentimentANN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(hidden_size, 2)  # 2 output classes (negative or positive)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "257b0968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.6999\n",
      "Epoch [2/10], Loss: 0.6969\n",
      "Epoch [3/10], Loss: 0.6934\n",
      "Epoch [4/10], Loss: 0.6900\n",
      "Epoch [5/10], Loss: 0.6858\n",
      "Epoch [6/10], Loss: 0.6815\n",
      "Epoch [7/10], Loss: 0.6767\n",
      "Epoch [8/10], Loss: 0.6714\n",
      "Epoch [9/10], Loss: 0.6661\n",
      "Epoch [10/10], Loss: 0.6604\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "input_size = train_x_tensor.shape[1]\n",
    "model = SentimentANN(input_size)\n",
    "\n",
    "# Loss & Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(train_x_tensor)\n",
    "    loss = criterion(outputs, train_y_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b669eb0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        47\n",
      "           1       0.53      1.00      0.69        53\n",
      "\n",
      "    accuracy                           0.53       100\n",
      "   macro avg       0.27      0.50      0.35       100\n",
      "weighted avg       0.28      0.53      0.37       100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\toste\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\toste\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\toste\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(validation_x_tensor)\n",
    "    predicted_labels = torch.argmax(predictions, axis=1)\n",
    "\n",
    "# Metrics\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(validation_y_tensor, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b77f93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task1.1 larger training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18292d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\toste\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\toste\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from matplotlib import pyplot\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, classification_report\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_pandas(data, columns):\n",
    "    df_ = pd.DataFrame(columns=columns)\n",
    "    data['Sentence'] = data['Sentence'].str.lower()\n",
    "    data['Sentence'] = data['Sentence'].replace('[a-zA-Z0-9-_.]+@[a-zA-Z0-9-_.]+', '', regex=True)                      # remove emails\n",
    "    data['Sentence'] = data['Sentence'].replace('((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.|$)){4}', '', regex=True)    # remove IP address\n",
    "    data['Sentence'] = data['Sentence'].str.replace('[^\\w\\s]', '', regex=True)  # NEW (explicit)                                                      # remove special characters\n",
    "    data['Sentence'] = data['Sentence'].replace('\\d', '', regex=True)                                                   # remove numbers\n",
    "    for index, row in data.iterrows():\n",
    "        word_tokens = word_tokenize(row['Sentence'])\n",
    "        filtered_sent = [w for w in word_tokens if not w in stopwords.words('english')]\n",
    "        df_.loc[len(df_)] = {\n",
    "            \"index\": row['index'],\n",
    "            \"Class\": row['Class'],\n",
    "            \"Sentence\": \" \".join(filtered_sent)\n",
    "        }\n",
    "    return data\n",
    "\n",
    "# If this is the primary file that is executed (ie not an import of another file)\n",
    "if __name__ == \"__main__\":\n",
    "    # get data, pre-process and split\n",
    "    file_path = r\"C:\\Users\\toste\\Downloads\\amazon_cells_labelled_LARGE_25K.txt\"\n",
    "    data = pd.read_csv(file_path, delimiter='\\t', header=None)\n",
    "    data.columns = ['Sentence', 'Class']\n",
    "    data['index'] = data.index                                          # add new column index\n",
    "    columns = ['index', 'Class', 'Sentence']\n",
    "    data = preprocess_pandas(data, columns)                             # pre-process\n",
    "    training_data, validation_data, training_labels, validation_labels = train_test_split( # split the data into training, validation, and test splits\n",
    "        data['Sentence'].values.astype('U'),\n",
    "        data['Class'].values.astype('int32'),\n",
    "        test_size=0.10,\n",
    "        random_state=0,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    # vectorize data using TFIDF and transform for PyTorch for scalability\n",
    "    word_vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1,2), max_features=50000, max_df=0.5, use_idf=True, norm='l2')\n",
    "    training_data = word_vectorizer.fit_transform(training_data)        # transform texts to sparse matrix\n",
    "    training_data = training_data.todense()                             # convert to dense matrix for Pytorch\n",
    "    vocab_size = len(word_vectorizer.vocabulary_)\n",
    "    validation_data = word_vectorizer.transform(validation_data)\n",
    "    validation_data = validation_data.todense()\n",
    "    train_x_tensor = torch.from_numpy(np.array(training_data)).type(torch.FloatTensor)\n",
    "    train_y_tensor = torch.from_numpy(np.array(training_labels)).long()\n",
    "    validation_x_tensor = torch.from_numpy(np.array(validation_data)).type(torch.FloatTensor)\n",
    "    validation_y_tensor = torch.from_numpy(np.array(validation_labels)).long()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5675d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "\n",
    "file_path = r\"C:\\Users\\toste\\Downloads\\amazon_cells_labelled_LARGE_25K.txt\"\n",
    "df = pd.read_csv(file_path, sep=\"\\t\", header=None, names=[\"text\", \"label\"])\n",
    "\n",
    "# Step 2: View the first few rows\n",
    "print(df.head())\n",
    "\n",
    "# Optional: Check distribution of labels\n",
    "print(df['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "133dbf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple feedforward neural network\n",
    "class SentimentANN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64):\n",
    "        super(SentimentANN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(hidden_size, 2)  # 2 output classes (negative or positive)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "300b9701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.6981\n",
      "Epoch [2/10], Loss: 0.6951\n",
      "Epoch [3/10], Loss: 0.6917\n",
      "Epoch [4/10], Loss: 0.6875\n",
      "Epoch [5/10], Loss: 0.6828\n",
      "Epoch [6/10], Loss: 0.6778\n",
      "Epoch [7/10], Loss: 0.6727\n",
      "Epoch [8/10], Loss: 0.6675\n",
      "Epoch [9/10], Loss: 0.6620\n",
      "Epoch [10/10], Loss: 0.6562\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "input_size = train_x_tensor.shape[1]\n",
    "model = SentimentANN(input_size)\n",
    "\n",
    "# Loss & Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(train_x_tensor)\n",
    "    loss = criterion(outputs, train_y_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d787d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.72      0.80      1011\n",
      "           1       0.83      0.95      0.89      1489\n",
      "\n",
      "    accuracy                           0.86      2500\n",
      "   macro avg       0.87      0.83      0.84      2500\n",
      "weighted avg       0.86      0.86      0.85      2500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(validation_x_tensor)\n",
    "    predicted_labels = torch.argmax(predictions, axis=1)\n",
    "\n",
    "# Metrics\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(validation_y_tensor, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f9f90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc6ec0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch 1/10 - Loss: 0.7602\n",
      "Epoch 2/10 - Loss: 1.8161\n",
      "Epoch 3/10 - Loss: 2.9878\n",
      "Epoch 4/10 - Loss: 1.1187\n",
      "Epoch 5/10 - Loss: 0.9864\n",
      "Epoch 6/10 - Loss: 0.6333\n",
      "Epoch 7/10 - Loss: 0.2949\n",
      "Epoch 8/10 - Loss: 0.4198\n",
      "Epoch 9/10 - Loss: 0.3079\n",
      "Epoch 10/10 - Loss: 0.1833\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.91      0.82        93\n",
      "           1       0.91      0.73      0.81       107\n",
      "\n",
      "    accuracy                           0.81       200\n",
      "   macro avg       0.83      0.82      0.81       200\n",
      "weighted avg       0.83      0.81      0.81       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Load the Amazon dataset\n",
    "file_path = r\"C:\\Users\\toste\\Downloads\\amazon_cells_labelled.txt\"\n",
    "df = pd.read_csv(file_path, sep='\\t', header=None, names=['text', 'label'])\n",
    "\n",
    "# 2. Vectorize text using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=512)\n",
    "X = vectorizer.fit_transform(df['text']).toarray()\n",
    "y = df['label'].values\n",
    "\n",
    "# 3. Split into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4. Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).unsqueeze(1)  # [batch, 1, emb]\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32).unsqueeze(1)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "# 5. Define a Transformer-based sentiment classifier\n",
    "class TransformerSentimentClassifier(nn.Module):\n",
    "    def __init__(self, dim=512, num_classes=2, num_heads=4, num_layers=2):\n",
    "        super().__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=dim, nhead=num_heads, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.classifier = nn.Linear(dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=1)  # mean over sequence\n",
    "        return self.classifier(x)\n",
    "\n",
    "# 6. Instantiate and train the model\n",
    "model = TransformerSentimentClassifier(dim=512)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "print(\"Training...\")\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/10 - Loss: {loss.item():.4f}\")\n",
    "\n",
    "# 7. Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_val_tensor)\n",
    "    preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, preds.numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a479068a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch 1/10 - Loss: 0.7679\n",
      "Epoch 2/10 - Loss: 4.5095\n",
      "Epoch 3/10 - Loss: 1.8153\n",
      "Epoch 4/10 - Loss: 1.9324\n",
      "Epoch 5/10 - Loss: 0.8216\n",
      "Epoch 6/10 - Loss: 1.0137\n",
      "Epoch 7/10 - Loss: 1.0012\n",
      "Epoch 8/10 - Loss: 0.6770\n",
      "Epoch 9/10 - Loss: 0.5360\n",
      "Epoch 10/10 - Loss: 0.6123\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.95      0.73      1926\n",
      "           1       0.95      0.59      0.73      3074\n",
      "\n",
      "    accuracy                           0.73      5000\n",
      "   macro avg       0.77      0.77      0.73      5000\n",
      "weighted avg       0.81      0.73      0.73      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Load the Amazon dataset\n",
    "file_path = r\"C:\\Users\\toste\\Downloads\\amazon_cells_labelled_LARGE_25K.txt\"\n",
    "df = pd.read_csv(file_path, sep='\\t', header=None, names=['text', 'label'])\n",
    "\n",
    "# 2. Vectorize text using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=512)\n",
    "X = vectorizer.fit_transform(df['text']).toarray()\n",
    "y = df['label'].values\n",
    "\n",
    "# 3. Split into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4. Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).unsqueeze(1)  # [batch, 1, emb]\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32).unsqueeze(1)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "# 5. Define a Transformer-based sentiment classifier\n",
    "class TransformerSentimentClassifier(nn.Module):\n",
    "    def __init__(self, dim=512, num_classes=2, num_heads=4, num_layers=2):\n",
    "        super().__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=dim, nhead=num_heads, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.classifier = nn.Linear(dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=1)  # mean over sequence\n",
    "        return self.classifier(x)\n",
    "\n",
    "# 6. Instantiate and train the model\n",
    "model = TransformerSentimentClassifier(dim=512)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "print(\"Training...\")\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/10 - Loss: {loss.item():.4f}\")\n",
    "\n",
    "# 7. Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_val_tensor)\n",
    "    preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, preds.numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ba67de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
