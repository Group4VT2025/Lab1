{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c72c5d70-c42c-42c1-a72d-54181f560f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 50000/50000 [00:58<00:00, 859.98 examples/s]\n",
      "Map: 100%|██████████| 10000/10000 [00:11<00:00, 860.44 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 / 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125 [06:53<00:00,  7.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished. Average Loss: 0.2152\n",
      "\n",
      "Epoch 2 / 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125 [07:00<00:00,  7.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 finished. Average Loss: 0.1133\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Import Libraries\n",
    "import torch\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from torch.optim import AdamW\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# STEP 2: Set Device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# STEP 3: Load the Amazon Polarity Dataset\n",
    "dataset = load_dataset(\"amazon_polarity\")\n",
    "\n",
    "# ✅ STEP 4: Subset the Dataset for Faster Training (50K train, 10K test)\n",
    "dataset[\"train\"] = dataset[\"train\"].select(range(50000))\n",
    "dataset[\"test\"] = dataset[\"test\"].select(range(10000))\n",
    "\n",
    "# STEP 5: Rename 'label' to 'labels' to work with Transformers\n",
    "dataset = dataset.rename_column(\"label\", \"labels\")\n",
    "\n",
    "# STEP 6: Load the BERT tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# ✅ STEP 7: Tokenization Function (max_length=128 for speed)\n",
    "def tokenize_function(batch):\n",
    "    return tokenizer(batch[\"content\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Apply tokenizer to the dataset\n",
    "dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# STEP 8: Set format for PyTorch (keep only required columns)\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# STEP 9: Create DataLoaders\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "train_loader = DataLoader(dataset[\"train\"], batch_size=16, shuffle=True, collate_fn=data_collator)\n",
    "val_loader = DataLoader(dataset[\"test\"], batch_size=16, shuffle=False, collate_fn=data_collator)\n",
    "\n",
    "# STEP 10: Load Pretrained BERT for Binary Classification\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "model = model.to(device)\n",
    "\n",
    "# STEP 11: Define Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# STEP 12: Training Loop\n",
    "num_epochs = 2\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    print(f\"\\nEpoch {epoch + 1} / {num_epochs}\")\n",
    "    \n",
    "    for batch in tqdm(train_loader):\n",
    "        # Move batch to the selected device (CPU/GPU)\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass and update\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print average loss for the epoch\n",
    "    print(f\"Epoch {epoch + 1} finished. Average Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # ✅ Optional: Save model after each epoch\n",
    "    model.save_pretrained(f\"./bert-amazon-epoch{epoch+1}\")\n",
    "    tokenizer.save_pretrained(f\"./bert-amazon-epoch{epoch+1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1ecd9c3-7633-44ba-be99-23b97445dde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since amazon_polarity couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'amazon_polarity' at /root/.cache/huggingface/datasets/amazon_polarity/amazon_polarity/0.0.0/9d9c45c18f8c3cf1b23a3c27917b60cbf28f3289 (last modified on Mon Apr 14 09:03:07 2025).\n",
      "Map: 100%|██████████| 500000/500000 [09:30<00:00, 877.15 examples/s]\n",
      "Map: 100%|██████████| 40000/40000 [00:44<00:00, 889.09 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 / 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31250/31250 [1:09:44<00:00,  7.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished. Average Loss: 0.1619\n",
      "\n",
      "Epoch 2 / 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31250/31250 [1:09:46<00:00,  7.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 finished. Average Loss: 0.1088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 2500/2500 [01:39<00:00, 25.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Evaluation Accuracy: 0.9495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Import Libraries\n",
    "import torch\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from torch.optim import AdamW\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# STEP 2: Set Device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# STEP 3: Load the Amazon Polarity Dataset\n",
    "dataset = load_dataset(\"amazon_polarity\")\n",
    "\n",
    "# ✅ STEP 4: Subset the Dataset for Faster Training (50K train, 10K test)\n",
    "dataset[\"train\"] = dataset[\"train\"].select(range(500000))\n",
    "dataset[\"test\"] = dataset[\"test\"].select(range(40000))\n",
    "\n",
    "# STEP 5: Rename 'label' to 'labels' to work with Transformers\n",
    "dataset = dataset.rename_column(\"label\", \"labels\")\n",
    "\n",
    "# STEP 6: Load the BERT tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# ✅ STEP 7: Tokenization Function (max_length=128 for speed)\n",
    "def tokenize_function(batch):\n",
    "    return tokenizer(batch[\"content\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Apply tokenizer to the dataset\n",
    "dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# STEP 8: Set format for PyTorch (keep only required columns)\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# STEP 9: Create DataLoaders\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "train_loader = DataLoader(dataset[\"train\"], batch_size=16, shuffle=True, collate_fn=data_collator)\n",
    "val_loader = DataLoader(dataset[\"test\"], batch_size=16, shuffle=False, collate_fn=data_collator)\n",
    "\n",
    "# STEP 10: Load Pretrained BERT for Binary Classification\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "model = model.to(device)\n",
    "\n",
    "# STEP 11: Define Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# STEP 12: Training Loop\n",
    "num_epochs = 2\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    print(f\"\\nEpoch {epoch + 1} / {num_epochs}\")\n",
    "    \n",
    "    for batch in tqdm(train_loader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} finished. Average Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # ✅ Optional: Save model after each epoch\n",
    "    model.save_pretrained(f\"./bert-amazon-epoch{epoch+1}\")\n",
    "    tokenizer.save_pretrained(f\"./bert-amazon-epoch{epoch+1}\")\n",
    "\n",
    "# STEP 13: Evaluation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        correct += (predictions == batch[\"labels\"]).sum().item()\n",
    "        total += batch[\"labels\"].size(0)\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"\\n✅ Evaluation Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a48cd414-b85d-4f39-8682-1c1e57284e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000000/1000000 [18:52<00:00, 882.98 examples/s]\n",
      "Map: 100%|██████████| 100000/100000 [01:53<00:00, 881.09 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 / 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62500/62500 [2:19:28<00:00,  7.47it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished. Average Loss: 0.1506\n",
      "\n",
      "Epoch 2 / 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62500/62500 [2:19:30<00:00,  7.47it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 finished. Average Loss: 0.1098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 6250/6250 [04:10<00:00, 24.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Evaluation Accuracy: 0.9509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Import Libraries\n",
    "import torch\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from torch.optim import AdamW\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# STEP 2: Set Device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# STEP 3: Load the Amazon Polarity Dataset\n",
    "dataset = load_dataset(\"amazon_polarity\")\n",
    "\n",
    "# ✅ STEP 4: Subset the Dataset for Faster Training (50K train, 10K test)\n",
    "dataset[\"train\"] = dataset[\"train\"].select(range(1000000))\n",
    "dataset[\"test\"] = dataset[\"test\"].select(range(100000))\n",
    "\n",
    "# STEP 5: Rename 'label' to 'labels' to work with Transformers\n",
    "dataset = dataset.rename_column(\"label\", \"labels\")\n",
    "\n",
    "# STEP 6: Load the BERT tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# ✅ STEP 7: Tokenization Function (max_length=128 for speed)\n",
    "def tokenize_function(batch):\n",
    "    return tokenizer(batch[\"content\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Apply tokenizer to the dataset\n",
    "dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# STEP 8: Set format for PyTorch (keep only required columns)\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# STEP 9: Create DataLoaders\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "train_loader = DataLoader(dataset[\"train\"], batch_size=16, shuffle=True, collate_fn=data_collator)\n",
    "val_loader = DataLoader(dataset[\"test\"], batch_size=16, shuffle=False, collate_fn=data_collator)\n",
    "\n",
    "# STEP 10: Load Pretrained BERT for Binary Classification\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "model = model.to(device)\n",
    "\n",
    "# STEP 11: Define Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# STEP 12: Training Loop\n",
    "num_epochs = 2\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    print(f\"\\nEpoch {epoch + 1} / {num_epochs}\")\n",
    "    \n",
    "    for batch in tqdm(train_loader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} finished. Average Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # ✅ Optional: Save model after each epoch\n",
    "    model.save_pretrained(f\"./bert-amazon-epoch{epoch+1}\")\n",
    "    tokenizer.save_pretrained(f\"./bert-amazon-epoch{epoch+1}\")\n",
    "\n",
    "# STEP 13: Evaluation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        correct += (predictions == batch[\"labels\"]).sum().item()\n",
    "        total += batch[\"labels\"].size(0)\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"\\n✅ Evaluation Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30e28f7e-ae57-475d-b69b-e223d3998343",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-04-15 07:31:07.463760: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744695067.482749    7281 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744695067.488513    7281 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744695067.504049    7281 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744695067.504066    7281 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744695067.504068    7281 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744695067.504070    7281 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-15 07:31:07.509464: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 / 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62500/62500 [2:19:19<00:00,  7.48it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished. Average Loss: 0.1504\n",
      "\n",
      "Epoch 2 / 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62500/62500 [2:19:34<00:00,  7.46it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 finished. Average Loss: 0.1097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 6250/6250 [04:09<00:00, 25.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluation Accuracy: 0.9516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Import Libraries\n",
    "import torch\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from torch.optim import AdamW\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Set GPU \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Load the Amazon Polarity Dataset\n",
    "dataset = load_dataset(\"amazon_polarity\")\n",
    "\n",
    "# Subset the Dataset for Faster Training (50K train, 10K test)\n",
    "dataset[\"train\"] = dataset[\"train\"].select(range(1000000))\n",
    "dataset[\"test\"] = dataset[\"test\"].select(range(100000))\n",
    "\n",
    "#Rename 'label' to 'labels' to work with Transformers\n",
    "dataset = dataset.rename_column(\"label\", \"labels\")\n",
    "\n",
    "#Load the BERT tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "#Tokenization Function (max_length=128 for speed)\n",
    "def tokenize_function(batch):\n",
    "    return tokenizer(batch[\"content\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Apply tokenizer to the dataset\n",
    "dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "#  Set format for PyTorch (keep only required columns)\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# Create DataLoaders\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "train_loader = DataLoader(dataset[\"train\"], batch_size=16, shuffle=True, collate_fn=data_collator)\n",
    "val_loader = DataLoader(dataset[\"test\"], batch_size=16, shuffle=False, collate_fn=data_collator)\n",
    "\n",
    "# Load Pretrained BERT for Binary Classification\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "model = model.to(device)\n",
    "\n",
    "# Define Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 2\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    print(f\"\\nEpoch {epoch + 1} / {num_epochs}\")\n",
    "    \n",
    "    for batch in tqdm(train_loader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} finished. Average Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    #Save model after each epoch\n",
    "    model.save_pretrained(f\"./bert-amazon-epoch{epoch+1}\")\n",
    "    tokenizer.save_pretrained(f\"./bert-amazon-epoch{epoch+1}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        correct += (predictions == batch[\"labels\"]).sum().item()\n",
    "        total += batch[\"labels\"].size(0)\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"\\n Evaluation Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd09873c-fe7a-459b-8165-e153b40db7d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
